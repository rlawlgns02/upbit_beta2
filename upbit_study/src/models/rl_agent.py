"""
ê°•í™”í•™ìŠµ íŠ¸ë ˆì´ë”© ì—ì´ì „íŠ¸
PPO(Proximal Policy Optimization) ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©
"""
import os
import numpy as np
import pandas as pd
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.callbacks import BaseCallback
from typing import Optional
import torch


class TradingAgent:
    """PPO ê¸°ë°˜ íŠ¸ë ˆì´ë”© ì—ì´ì „íŠ¸"""

    def __init__(self,
                 env,
                 model_name: str = "crypto_trader",
                 learning_rate: float = 0.0003,
                 n_steps: int = 2048,
                 batch_size: int = 64,
                 n_epochs: int = 10,
                 gamma: float = 0.99,
                 device: str = 'auto',
                 use_tensorboard: bool = False):
        """
        Args:
            env: Gymnasium í™˜ê²½
            model_name: ëª¨ë¸ ì´ë¦„
            learning_rate: í•™ìŠµë¥ 
            n_steps: ì—…ë°ì´íŠ¸ ì „ ìˆ˜ì§‘í•  ìŠ¤í… ìˆ˜
            batch_size: ë°°ì¹˜ í¬ê¸°
            n_epochs: ì—í¬í¬ ìˆ˜
            gamma: í• ì¸ ê³„ìˆ˜
            device: 'cpu', 'cuda', 'auto'
            use_tensorboard: TensorBoard ë¡œê¹… ì‚¬ìš© ì—¬ë¶€
        """
        self.env = DummyVecEnv([lambda: env])
        self.model_name = model_name

        # TensorBoard ë¡œê¹… ì„¤ì • (ì„ íƒì )
        tensorboard_log = "./logs/tensorboard/" if use_tensorboard else None

        # PPO ëª¨ë¸ ìƒì„±
        self.model = PPO(
            "MlpPolicy",
            self.env,
            learning_rate=learning_rate,
            n_steps=n_steps,
            batch_size=batch_size,
            n_epochs=n_epochs,
            gamma=gamma,
            verbose=1,
            device=device,
            tensorboard_log=tensorboard_log
        )

    def train(self, total_timesteps: int = 100000, callback: Optional[BaseCallback] = None):
        """ëª¨ë¸ í•™ìŠµ

        Args:
            total_timesteps: ì´ í•™ìŠµ ìŠ¤í…
            callback: ì½œë°± í•¨ìˆ˜
        """
        print(f"ğŸ¯ í•™ìŠµ ì‹œì‘: {total_timesteps:,} ìŠ¤í…")
        self.model.learn(
            total_timesteps=total_timesteps,
            callback=callback,
            progress_bar=True
        )
        print("âœ… í•™ìŠµ ì™„ë£Œ!")

    def predict(self, observation: np.ndarray, deterministic: bool = True):
        """ì•¡ì…˜ ì˜ˆì¸¡

        Args:
            observation: í˜„ì¬ ê´€ì¸¡ê°’
            deterministic: ê²°ì •ì  ì˜ˆì¸¡ ì—¬ë¶€

        Returns:
            action, _states
        """
        return self.model.predict(observation, deterministic=deterministic)

    def save(self, path: Optional[str] = None):
        """ëª¨ë¸ ì €ì¥

        Args:
            path: ì €ì¥ ê²½ë¡œ (Noneì´ë©´ ê¸°ë³¸ ê²½ë¡œ)
        """
        if path is None:
            os.makedirs("models", exist_ok=True)
            path = f"models/{self.model_name}"

        self.model.save(path)
        print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥: {path}")

    def load(self, path: Optional[str] = None):
        """ëª¨ë¸ ë¡œë“œ

        Args:
            path: ë¡œë“œ ê²½ë¡œ (Noneì´ë©´ ê¸°ë³¸ ê²½ë¡œ)
        """
        if path is None:
            path = f"models/{self.model_name}"

        self.model = PPO.load(path, env=self.env)
        print(f"ğŸ“‚ ëª¨ë¸ ë¡œë“œ: {path}")

    @staticmethod
    def get_device() -> str:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
        if torch.cuda.is_available():
            return "cuda"
        elif torch.backends.mps.is_available():
            return "mps"
        else:
            return "cpu"


class TrainingCallback(BaseCallback):
    """í•™ìŠµ ì¤‘ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì½œë°±"""

    def __init__(self, check_freq: int = 1000, save_path: str = "logs/"):
        """
        Args:
            check_freq: ì²´í¬ ë¹ˆë„
            save_path: ë¡œê·¸ ì €ì¥ ê²½ë¡œ
        """
        super().__init__()
        self.check_freq = check_freq
        self.save_path = save_path
        os.makedirs(save_path, exist_ok=True)

        self.episode_rewards = []
        self.episode_lengths = []

    def _on_step(self) -> bool:
        """ê° ìŠ¤í…ë§ˆë‹¤ í˜¸ì¶œ"""
        if self.n_calls % self.check_freq == 0:
            # ì—í”¼ì†Œë“œ ë³´ìƒ ê¸°ë¡
            if len(self.model.ep_info_buffer) > 0:
                mean_reward = np.mean([ep_info["r"] for ep_info in self.model.ep_info_buffer])
                mean_length = np.mean([ep_info["l"] for ep_info in self.model.ep_info_buffer])

                self.episode_rewards.append(mean_reward)
                self.episode_lengths.append(mean_length)

                print(f"\nğŸ“Š Step {self.n_calls:,}")
                print(f"   í‰ê·  ë³´ìƒ: {mean_reward:.4f}")
                print(f"   í‰ê·  ê¸¸ì´: {mean_length:.0f}")

        return True


def evaluate_agent(agent: TradingAgent, env, n_episodes: int = 10) -> dict:
    """ì—ì´ì „íŠ¸ ì„±ëŠ¥ í‰ê°€

    Args:
        agent: íŠ¸ë ˆì´ë”© ì—ì´ì „íŠ¸
        env: í‰ê°€ í™˜ê²½
        n_episodes: í‰ê°€ ì—í”¼ì†Œë“œ ìˆ˜

    Returns:
        í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    episode_rewards = []
    episode_profits = []
    episode_trades = []

    for episode in range(n_episodes):
        obs, _ = env.reset()
        done = False
        episode_reward = 0

        while not done:
            action, _ = agent.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            episode_reward += reward

        episode_rewards.append(episode_reward)
        episode_profits.append(info['total_profit'])
        episode_trades.append(info['total_trades'])

    results = {
        'mean_reward': np.mean(episode_rewards),
        'std_reward': np.std(episode_rewards),
        'mean_profit': np.mean(episode_profits),
        'std_profit': np.std(episode_profits),
        'mean_trades': np.mean(episode_trades),
        'win_rate': sum(1 for p in episode_profits if p > 0) / n_episodes
    }

    print("\nğŸ“ˆ í‰ê°€ ê²°ê³¼:")
    print(f"   í‰ê·  ë³´ìƒ: {results['mean_reward']:.4f} Â± {results['std_reward']:.4f}")
    print(f"   í‰ê·  ìˆ˜ìµ: {results['mean_profit']:,.0f} Â± {results['std_profit']:,.0f} KRW")
    print(f"   í‰ê·  ê±°ë˜ íšŸìˆ˜: {results['mean_trades']:.0f}")
    print(f"   ìŠ¹ë¥ : {results['win_rate']*100:.1f}%")

    return results
